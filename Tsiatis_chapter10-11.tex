\documentclass[xcolor=dvipsnames,aspectratio=169]{beamer}
\usetheme{metropolis}
\usefonttheme{professionalfonts}
\setbeamertemplate{theorems}[numbered]

\usepackage{newtxtext}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{ascmac}
\everymath{\displaystyle}

\newtheorem{thm}{Theorem}[section]
\newtheorem{ex}[thm]{Example}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}

\newcommand{\indep}{\mathop{\perp\!\!\!\!\perp}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\1}{\mathbbm{1}}

\title{Tsiatis (2006) Chapters 10--11\\ \small{Asymptotic Statistics 2025 Summer Reading Group}}
\author{Naoki Eguchi}
\institute{Faculty of Medicine, Kyoto University}
\date{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Today's agenda}
  \begin{itemize}
    \item Recap: Coarsened data, IF geometry, AIPWCC in one slide
    \item Chapter 10: Two-level missingness $\Rightarrow$ efficient AIPWCC \& DR
    \item \emph{Skip (brief mention only):} monotone coarsening details, censoring
    \item Generalization idea (operators $L,\,M,\,M^{-1}$) for later use
    \item Chapter 11: Locally efficient estimators (two representations; how to compute)
    \item Practical recipe: what to implement in 2h
  \end{itemize}
\end{frame}

\section{Recap}

\begin{frame}{Coarsened data and Semiparametric statistics}
  \begin{itemize}
    \item Full data $Z$; observed data $\{C,\,G_C(Z)\}$ under CAR (coarsening at random).\\
          $\Rightarrow\ \forall r,\ P(C=r\mid Z)=\pi\{r,\,G_r(Z)\}$ \ (coarsening depends only on observed data.)
    \item When $C=\infty$, the data are completely observed (equal to full data).
    \item In semiparametric statistics, we study the influence function, an element of $\Lambda^{\perp}$.
    \begin{itemize}
      \item Full-data tangent space: $\mathcal{H}^F$; full-data nuisance tangent space: $\Lambda_F \subset \mathcal{H}^F$.
      \begin{itemize}
        \item (Observed) tangent space: $\mathcal{H}$; nuisance tangent space:
              $\Lambda=\Lambda_{\psi}\oplus \Lambda_{\eta}\ \ (\Lambda_{\psi}\perp \Lambda_{\eta})$.
      \end{itemize}
    \end{itemize}
    \item \textbf{Theorem 8.3}: all observed-data influence functions can be written as
    \[
      \varphi \{C,G_C(Z)\}
      =
      \Biggl[
        \frac{\mathbf{1}(C=\infty)}{\pi(\infty,Z,\psi_0)}\,\varphi ^F(Z)
        + L_2\{C,G_C(Z)\}
      \Biggr]
      \;-\; \Pi\!\bigl([\cdot]\mid \Lambda_\psi\bigr)
    \]
    where $\varphi ^F(Z)$ : full-data IF, $L_2\{C,G_C(Z)\}\in \Lambda_2$ (augmentation space)
  \end{itemize}
\end{frame}

\section{10. Improving Efficiency \& Double Robustness with Coarsened Data}

\begin{frame}{The optimal (variance-minimizing) observed-data IF}
    \begin{tcolorbox}[colframe=Cyan,title=Theorem 10.1]
  \begin{itemize}
    \item For fixed full-data IF $\varphi_F(Z)\in(\mathrm{IF})_F$, the optimal choice is
    \[
      L_2\{C,G_C(Z)\}
      \;=\;
      -\,\Pi\!\left[
        \frac{\mathbf{1}(C=\infty)\,\varphi_F(Z)}{\pi(\infty,Z,\psi_0)}
        \ \middle|\ \Lambda_2
      \right].
    \]
    \item Hence the optimal observed-data influence function is given by
    \[
      \frac{\mathbf{1}(C=\infty)\,\varphi_F(Z)}{\pi(\infty,Z,\psi_0)}
      \;-\;
      \Pi\!\left[
        \frac{\mathbf{1}(C=\infty)\,\varphi_F(Z)}{\pi(\infty,Z,\psi_0)}
        \ \middle|\ \Lambda_2
      \right].
    \]
  \end{itemize}
\end{tcolorbox}
\begin{itemize}
    \item \textit{Remark 1} : If we \alert{know $\psi_0$}, we can choose optimal IF. \\
    Also, we do not have to estimate $\psi$ because $\Lambda_{\psi}\subset \Lambda_2$ (already subtracted !)
\end{itemize}
\end{frame}

\begin{frame}{Generalized by linear operator}
    \begin{tcolorbox}[colframe=red,title=Definition 1]
    \begin{itemize}
      \item Define the linear operator $\mathcal{J}:\mathcal{H}^F\to\mathcal{H}$
      \[
        h_F \mapsto 
        \mathcal{J}
        (h_F) =
        \frac{\mathbf{1}(C=\infty)}{\pi(\infty,Z,\psi_0)}\,h_F(Z)
        - \Pi\!\left[
          \frac{\mathbf{1}(C=\infty)}{\pi(\infty,Z,\psi_0)}\,h_F(Z)
          \ \middle|\ \Lambda_2
        \right].
      \]
    \end{itemize}
  \end{tcolorbox}
  \begin{itemize}
    \item This operator turns full-data IF into efficient observed-data IF, so we can create efficient observed-data IF space.
  \end{itemize}
  \begin{tcolorbox}[colframe=red,title=Definition 2]
    \begin{itemize}
      \item The class of \emph{double-robust} observed-data influence functions is defined by
      \[
        (\mathrm{IF})_{\mathrm{DR}}
        =
        \big\{\, \mathcal{J}(\varphi_F): \varphi_F(Z)\in(\mathrm{IF})_F \,\big\}.
      \]
    \end{itemize}
  \end{tcolorbox}
\end{frame}

\begin{frame}{DR linear space}
    \begin{itemize}
      \item $(\mathrm{IF})^F=\varphi^F(Z)+\mathcal{T}^{F\perp}$ is a linear variety in $\mathcal{H}^F$.
      \item Since $\mathcal{J}$ is \alert{linear} operator, $(\mathrm{IF})_{\mathrm{DR}}=\mathcal{J}\{(\mathrm{IF})^F\}=\mathcal{J}(\varphi^F)+\mathcal{J}(\mathcal{T}^{F\perp})$ in $\mathcal{H}$.
    \end{itemize}
    \begin{tcolorbox}[colframe=red,title=Definition 3]
    \begin{itemize}
      \item The linear subspace
      \[
        \mathcal{J}(\Lambda^{F\perp}) \;\subset\; \Lambda^{\perp} \;\subset\; \mathcal{H},
      \]
      is called the \textbf{DR linear space}.
      \item Explicitly,
      \[
        \mathcal{J}(\Lambda^{F\perp})
        =
        \Bigl\{\, \mathcal{J}(\varphi^{F}) : \varphi^{F}(Z)\in\Lambda^{F\perp} \,\Bigr\},
      \]
    \end{itemize}
  \end{tcolorbox}
\end{frame}

\section{10.2 Improving Efficiency with Two Levels of Missingness}

\begin{frame}{Set-up: Two-level missingness}
  \begin{itemize}
    \item Partition $Z=(Z_1^\top,Z_2^\top)^\top$ with $Z_1$ always observed and $Z_2$ sometimes missing.
    \item Complete-case indicator $R\in\{0,1\}$ with MAR: \\ $P(R=1\mid Z)=P(R=1\mid Z_1)=\pi(Z_1,\psi)$ (e.g., logit model).
    \item Observed data: $O=(R, Z_1, RZ_2)$.
    \item In this case, $\frac{I(C=\infty)\,\varphi^{\ast F}(Z)}
     {\varpi(\infty,Z)}=\frac{R\varphi^{\ast F}(Z)}{\pi(Z_1)}$ for $\varphi^{\ast F}(Z)=m(Z,\beta_0)\in \Lambda^{F\perp}$, hence, we want to find its projection onto $\Lambda_2$.
    \item Also, $\Lambda_2=\{\frac{R-\pi(Z_1)}{\pi(Z_1)}\,h_2(Z_1,\beta) : \forall h_2 : Z_1\rightarrow \mathbb{R}^q\}$
  \end{itemize}
\end{frame}

\begin{frame}{Finding the Projection onto the Augmentation space}
  \begin{tcolorbox}[colframe=Cyan,title=Theorem 10.2]
    \begin{itemize}
      \item The projection of 
      \[
        \frac{R\,\varphi^{\ast F}(Z)}{\pi(Z_{1})}
      \]
      onto the augmentation space $\Lambda_{2}$ is the unique element
      \[
        \Biggl(\frac{R-\pi(Z_{1})}{\pi(Z_{1})}\Biggr)h^{0}_{2}(Z_{1}) \;\in\; \Lambda_{2},
      \]
      where
      \[
        h^{0}_{2}(Z_{1}) = \E\!\left[\varphi^{\ast F}(Z)\mid Z_{1}\right].
      \]
    \end{itemize}
  \end{tcolorbox}
  \begin{itemize}
    \item For implementaion, we just compute $\E\!\left[\varphi^{\ast F}(Z)\mid Z_{1}\right]$. 
  \end{itemize}
\end{frame}

\begin{frame}{Adaptive estimation and its algorithm}
  \begin{itemize}
    \item Consider the case where we can \alert{choose} an estimating function $m(Z,\beta)$.
    \item \textbf{Optimal} augmentation: $h_2^\star(Z_1,\beta)=\E\!\left[m(Z,\beta)\mid Z_1\right]$.
    \item In practice, posit a (possibly misspecified) model $p^\ast_{Z_2|Z_1}(\cdot\mid z_1;\xi)$ and compute
    \[
      h_2^\ast(Z_1,\beta,\xi)=\int m(z_1,u,\beta)\,p^\ast_{Z_2|Z_1}(u\mid z_1;\xi)\,du.
    \]
    \item Estimation: fit $\hat\psi$ by MLE for $\pi(\cdot,\psi)$; fit $\hat\xi^\ast$ by likelihood for the posited model; solve
    \[
      \sum_{i=1}^n\left[\frac{R_i}{\pi(Z_{1i},\hat\psi_n)}m(Z_i,\beta)
      -\frac{R_i-\pi(Z_{1i},\hat\psi_n)}{\pi(Z_{1i},\hat\psi_n)}\,h_2^\ast(Z_{1i},\beta,\hat\xi_n^\ast)\right]=0.
    \]
    \item This estimation strategy is relatively easy to implement.
  \end{itemize}
\end{frame}

\begin{frame}{AIPWCC estimation which is igronrant of $\xi$}
  \begin{itemize}
    \item While, using posit model, we can think of the limiting value $h_2^*(Z_{1i},\beta, \xi^*)$.
    \item Even if $h_2^*(Z_{1i},\beta, \xi^*)\neq h_2^0(Z_{1i},\beta)$, it is a function of $Z_{1i}$, which satisfies the condition that can construct the IF.
    \[
    -\left\{
  \frac{R_{i}-\pi(Z_{1i},\psi_{0})}{\pi(Z_{1i},\psi_{0})}
 \right\}
 h^{\ast}_{2}(Z_{1i},\beta_{0},\xi^{\ast})
 \;\in\;\Lambda_{2}
    \]
    \item Therefore, the estimating equation for IF is
    \[
    \sum_{i=1}^{n}
\left[
  \frac{R_{i}\,m(Z_{i},\beta)}{\pi(Z_{1i},\hat{\psi}_{n})}
  - \left\{
      \frac{R_{i}-\pi(Z_{1i},\hat{\psi}_{n})}{\pi(Z_{1i},\hat{\psi}_{n})}
    \right\}
    h^{\ast}_{2}(Z_{1i},\beta_{0},\xi^{\ast})
\right]
= 0
    \]
    \item The only difference is to estimate $\xi_n^*$ or not (AIPWCC requires integration).
  \end{itemize}
\end{frame}

\begin{frame}{Asymptotic equivalence between adaptive estimation and AIPWCC}
  \begin{itemize}
    \begin{tcolorbox}[colframe=Cyan,title=Theorem 10.3]
    Let $\widehat{\beta}_n$ be the estimator obtained by the adaptive estimation, and let $\widehat{\beta}_n^{\ast}$ be the AIPWCC estimator
    . Under suitable regularity conditions,
    \[
      n^{1/2}\!\left(\widehat{\beta}_n-\widehat{\beta}_n^{\ast}\right)\xrightarrow{P}0.
    \]
    Hence, the adaptive estimator is asymptotically equivalent to the
    AIPWCC estimator.
  \end{tcolorbox}
  \item Thus, we think misspecification of $\xi$ and its estimation (=adaptive estimation) as a kind of negliguble problem. (BUT no efficiency !)
  \end{itemize}
\end{frame}

\begin{frame}{Double robustness (DR)}
  \begin{itemize}
    \item Under regularity, the adaptive AIPWCC estimator is \textbf{consistent} if
    \[
      \textit{either}\ \ \pi(Z_1,\psi)\ \text{is correctly specified} \quad \textbf{or}\quad p^\ast_{Z_2|Z_1}(\cdot\mid Z_1;\xi)\ \text{is correct}.
    \]
    \item Proof sketch for monotone coarsening: algebraic identity + CAR $\Rightarrow$ decompositions where either model kills the bias term.
  \end{itemize}
  \begin{tcolorbox}[title=Takeaway]
  Adaptive augmentation buys efficiency; the DR structure buys protection against one misspecification.
  \end{tcolorbox}
  % DR statement and identity underpinning (10.62)–(10.65) and Theorem 10.5
  % :contentReference[oaicite:19]{index=19}  :contentReference[oaicite:20]{index=20}
\end{frame}

\begin{frame}{Worked example (logistic regression; design-based missingness)}
  \begin{itemize}
    \item Full-data $m(Z,\beta)$ chosen for logistic model of $Y$ on $X=(X_1,X_2)$.
    \item Under two-level missingness, the AIPWCC form becomes (schematically)
    \[
      \sum_i\left[\frac{R_i}{\pi(Y_i,X_{1i};\hat\psi)}\,X^\ast_i\{Y_i-\mu_\beta(X_i)\}
      -\frac{R_i-\pi(Y_i,X_{1i};\hat\psi)}{\pi(Y_i,X_{1i};\hat\psi)}\,L(Y_i,X_{1i})\right]=0,
    \]
    with $L^\star(Y,X_1)=\E\!\left[X^\ast\{Y-\mu_\beta(X)\}\mid Y,X_1\right]$.
  \end{itemize}
  % Logistic revisited, (10.38)–(10.39); L* as conditional mean given (Y,X1)
  % :contentReference[oaicite:21]{index=21}
\end{frame}

\begin{frame}{What we skip in Ch.~10 (for time)}
  \begin{itemize}
    \item Detailed development for \emph{monotone coarsening} and for \emph{censoring} (survival)---we’ll pick up the tools where needed in Ch.~11.
    \item Nonmonotone coarsening exists but is computationally heavy (iterative projections, integral operators).
  \end{itemize}
  \begin{tcolorbox}[title=Pointer]
    Keep the operators $L$ and $M$ in mind; they will be our bridge to Chapter~11.
  \end{tcolorbox}
\end{frame}

\section{10.5 Improving Efficiency when Coarsening is Nonmonotone}

\begin{frame}{Generalization idea: operators $L$ and $M$ (you’ll need this)}
  \begin{tcolorbox}[colframe=red,title=Definition 4 and 5]
    \begin{itemize}
      \item Define two linear operators $\mathcal{L}:\mathcal{H}^{F}\to\mathcal{H}$ and  $\mathcal{M}:\mathcal{H}^{F}\to\mathcal{H}^{F}$
      \[
        \mathcal{L}\{h^{F}\} = \E\!\left[h^{F}(Z)\mid C,G_C(Z)\right],
        \qquad
        \mathcal{M}\{h^{F}\} = \E\!\left[\mathcal{L}\{h^{F}\}\mid Z\right].
      \]
      where $h^F$ is an arbitrary element in $\mathcal{H}^F$.
    \end{itemize}
  \end{tcolorbox}
    \[
        L\{h_F(\cdot)\} = \sum_{r=1}^{\infty} I(C=r)\,\E\{h_F(Z)\mid G_r(Z)\},
    \]
    and 
    \[
        M\{h_F(\cdot)\} = \sum_{r=1}^{\infty} \#\{r,G_r(Z)\}\,\E\{h_F(Z)\mid G_r(Z)\}.
    \]
\end{frame}

\begin{frame}{How to derive the projection : $\Pi\!\left[
        \frac{\mathbf{1}(C=\infty)h_F(Z)}{\pi(\infty,Z)}
        \,\Bigg|\, \Lambda_2
        \right]$}
  \begin{tcolorbox}[colframe=Cyan,title=Theorem 10.6]
    \begin{itemize}
      \item Inverse operator $\mathcal{M}^{-1}$ exists and is uniquely defined. 
      \item Moreover, the projection is given by
      \[
        \Pi\!\left[
        \frac{\mathbf{1}(C=\infty)h_F(Z)}{\pi(\infty,Z)}
        \,\Bigg|\, \Lambda_2
        \right]
        = \frac{\mathcal{1} (C=\infty)h_F(Z)}{\pi(\infty,Z)} 
        - \mathcal{L}\!\big[\,\mathcal{M}^{-1}\{h_F(\cdot)\}\big],
    \]
    \end{itemize}
  \end{tcolorbox}
  \begin{itemize}
    \item By \textbf{Definition 1}, $\mathcal{L}\!\big[\,\mathcal{M}^{-1}\{\varphi^F(Z)\}\big]=\mathcal{J}\{\varphi^F(Z)\}$.
    \item Therefore, DR linear space is written by $\mathcal{J}(\Lambda^{F\perp})=\mathcal{L}\!\big[\,\mathcal{M}^{-1}\{\Lambda^{F\perp}\}\big]$.
  \end{itemize}
\end{frame}

\begin{frame}{Proof sketch for the uniqueness of $\mathcal{M}^{-1}$}
  \begin{itemize}
    \item $I-\mathcal{M}$ is a contraction mapping 
    \[
    \Leftrightarrow \forall h^F\in\mathcal{H}^F, 0\leq \exists L<1, ||(I-\mathcal{M})h^F||\leq L||h^F||
    \]
    \item Existence of $\mathcal{M}^{-1}$ : $\sum_{k=0}^{\infty}(I-\mathcal{M})^k$ (Completeness of linear operator space)
    \begin{align*}
      \because \sum_{k=0}^n(I-\mathcal{M})^k\circ \mathcal{M}=I-(I-\mathcal{M})^{n+1}\rightarrow I \tag{as $n\to \infty$}
    \end{align*}
    \item Uniqueness of $\mathcal{M}^{-1}$ : $\mathcal{M}^{-1}=\mathcal{M}^{-1}(\mathcal{M}\circ \mathcal{M'}^{-1})=(\mathcal{M}^{-1}\circ \mathcal{M})\mathcal{M'}^{-1}=\mathcal{M'}^{-1}$
  \end{itemize}
\end{frame}

\begin{frame}{Constructing Strategy}
  \begin{itemize}
    \item we only focus on the operators $\mathcal{L}, \mathcal{M}^{-1}$ to estimate $\beta$.
    \[
    \sum_{i=1}^{n}
  \mathcal{L}_{i}\!\left[
    \mathcal{M}_{i}^{-1}\{\, m(Z_{i},\beta)\,\}
  \right]
= 0.
    \]
    \begin{itemize}
      \item $\mathcal{L}$ : posit model $\xi$ for conditional probability of $Z$ given $G_r(Z)$ → $\mathcal{L}(\cdot, \xi)$
      \item $\mathcal{M}^{-1}$ : $\psi$ for coarsening probability, posit model $\xi$ (same above) → $\mathcal{M}^{-1}(\cdot, \psi, \xi)$
    \end{itemize}
    \item To construct $\beta$, after estimating $\psi$ for $\hat{\psi_n}$, and $\xi$ for $\hat{\xi_n^*}$, solve the equation
    \[
    \sum_{i=1}^{n}
  \mathcal{L}_{i}\!\left[
    \mathcal{M}_{i}^{-1}\{\, m(Z_{i},\beta),\, \hat{\psi}_{n},\, \hat{\xi}^{\ast}_{n}\,\},
    \, \hat{\xi}^{\ast}_{n}
  \right]
= 0
    \]
  \end{itemize}
\end{frame}

\begin{frame}{Equivalence of $\mathcal{L}\circ \mathcal{M}^{-1}$ and AIPWCC}
  \begin{tcolorbox}[colframe=Cyan,title=Theorem 10.7 - (i)]
    \begin{itemize}
        \item Let $d^F(Z,\beta,\psi,\xi) = \mathcal{M}^{-1}\{m(Z,\beta),\psi,\xi\}$, then, the residual can separate
        \begin{align*}
          \mathcal{L}[\mathcal{M}^{-1}\{m(Z,\beta),\psi,\xi\},\xi]
            &= \mathcal{L}\{d_F(Z,\beta,\psi,\xi),\xi\} \\
            &=\underbrace{\frac{I(C=\infty)\,m(Z,\beta)}{\pi(\infty,Z,\psi)}}_{\text{IPWCC space}}
            + \underbrace{L^{\ast}_2\{C,G_C(Z),\beta,\psi,\xi\}}_{\text{Augmentation space}}.
        \end{align*}
    \end{itemize}
\end{tcolorbox}
  \begin{itemize}
    \item This means the solution to this estimating equation is an AIPWCC estimator
    \[
    \sum_{i=1}^{n}
  \frac{I(C_{i}=\infty)\,m(Z_{i},\beta)}
       {\varpi(\infty,Z_{i},\hat{\psi}_{n})}
  + L^{\ast}_{2}\{\,C_{i},G_{C_{i}}(Z_{i}),\beta,\hat{\psi}_{n},\hat{\xi}^{\ast}_{n}\}
= 0.
    \]
  \end{itemize}
\end{frame}

\begin{frame}{The part of augmentation space}
  \begin{tcolorbox}[colframe=Cyan,title=Theorem 10.7 - (ii)]
    \begin{itemize}
        \item The element of the augmentation space is written by
        \begin{align*}
          &L^{\ast}_2\{C,G_C(Z),\beta,\psi,\xi\}
            =\\  &- \frac{I(C=\infty)}{\pi(\infty,Z,\psi)}
            \Bigg(\sum_{r\neq\infty}\pi(r,G_r(Z),\psi)\,
            \E[d_F(Z,\beta,\psi,\xi)\mid G_r(Z),\xi]\Bigg) \\
            &+ \sum_{r\neq\infty} I(C=r)\,
            \E[d_F(Z,\beta,\psi,\xi)\mid G_r(Z),\xi]
        \end{align*}
    \end{itemize}
\end{tcolorbox}
\begin{itemize}
  \item Let $L_{2r}\{G_r(Z)\}=-\E[d_F(Z,\beta,\psi,\xi)\mid G_r(Z),\xi]$, we can obtain the same representation as a typical element of $\Lambda_2$. (see (7.37))
\end{itemize}
\end{frame}

\begin{frame}{Construct $\mathcal{M}^{-1}$ and $\mathcal{L}$}
  \begin{itemize}
    \item $\mathcal{M}^{-1}$ is represented by infinite series, so be constructed by successive approximation.
    \[
    d^F_{(j)}(Z,\beta,\psi,\xi) = \hat{\mathcal{M}}^{-1}\{m(Z,\beta),\psi,\xi\}
    \]
    \item By construction, $L^{\ast}_2\{C,G_C(Z),\beta,\psi,\xi\}\in \Lambda_2$ in any cases.
    \item This means that when $\psi_0$ (coarsening probability model) is correctly specified, 
    \[
    \frac{I(C=\infty)\,m(Z,\beta_{0})}{\varpi(\infty,Z,\psi_{0})}
\;+\;
L^{\ast}_{2(j)}\{\,C,G_{C}(Z),\beta_{0},\psi_{0},\xi^{\ast}\}\in \Lambda^{\perp}
    \]
    \item Hence, we can obtain an AIPWCC estimator by solving
    \[
    \sum_{i=1}^{n}
  \frac{I(C_{i}=\infty)\,m(Z_{i},\beta)}
       {\varpi(\infty,Z_{i},\hat{\psi}_{n})}
  + L^{\ast}_{2(j)}\{\,C_{i},G_{C_{i}}(Z_{i}),\beta,\hat{\psi}_{n},\hat{\xi}^{\ast}_{n}\}
= 0.
    \]
    \item The estimator is RAL (i.e. consistent and asymptotically normal)
  \end{itemize}
\end{frame}

\begin{frame}{Double Robustness}
  \begin{itemize}
    \item In addition to correct specification of $\psi$, the posit model $p_Z^*(z,\xi)$ is correctly specified, its influence function is \alert{efficient}. (double-robustness)
    \item Also, this AIPWCC estimator is consistent and asymptotically normal even if $\psi$ is misspecified. (Actually, we do not have to consider this pattern \alert{thanks to CAR})
    \item Recall this two probabilistic model
    \begin{itemize}
      \item posit model $p_Z^*(z,\xi)$; estimate $\hat{\xi}_n^* \rightarrow \xi^*(\neq \xi_0)$ (if misspecified)
      \item coarsening model $\pi\{r, G_r(Z),\psi\}$; estimate $\hat{\psi}_n^*\rightarrow \psi^*(\neq \psi_0)$ (if misspecified)
    \end{itemize} 
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{tcolorbox}[colframe=Cyan,title=Theorem 10.8]
    \begin{itemize}
      \item This theorem establishes the \textbf{double robustness} property.
      \item Specifically:
      \[
        \mathbb{E}_{\xi_0,\psi_0}
        \left[
          L\!\left(M^{-1}\{m(Z,\beta_0),\psi_0,\xi^\ast\},\,\xi^\ast\right)
        \right] = 0,
      \]
      and
      \[
        \mathbb{E}_{\xi_0,\psi_0}
        \left[
          L\!\left(M^{-1}\{m(Z,\beta_0),\psi^\ast,\xi_0\},\,\xi_0\right)
        \right] = 0.
      \]
      \item Hence, the estimating equation remains unbiased if either
      the coarsening model ($\psi$) or the marginal model of $Z$ ($\xi$) is correctly specified.
    \end{itemize}
  \end{tcolorbox}
\end{frame}

\section{11. Locally Efficient Estimators for Coarsened-Data Semiparametric Models}

\begin{frame}{Roadmap for Chapter 11}
  \begin{itemize}
    \item For a full-data estimating function $m(Z,\beta)\in \Lambda^{F\perp}$, applying $\mathcal{J}$, we obtain an observed-data estimating function.
    \[
    \sum_{i=1}^n \left[
  \frac{ I(C_i = \infty) m(Z_i,\beta) }
       { \varpi(\infty, Z_i, \hat{\psi}_n) }
  - \Pi\!\left(
    \frac{ I(C_i = \infty) m(Z_i,\beta) }
         { \varpi(\infty, Z_i) }
    \;\middle|\; \Lambda_2
  \right)
\right] = 0
    \]
    \item For computation, we need to estimate $\psi$ in the former, $\xi$ (posit model) inthe latter.
    \item We propose a locally efficient estimator, which achieves the semiparametric efficiency bound if the posited model $p_Z^*(z,\xi)$ is correct but still be consistent and asymptotically normal and RAL even if misspecified.
    \item Recall the essential framework : Efficient score → Efficient IF (proportional relationship)
  \end{itemize}
\end{frame}

\section{11.1 The Observed-Data Efficient Score}

\begin{frame}{Efficient score: Representation 1 (likelihood-based) : theoritical}
  \begin{itemize}
    \item Basiccaly, the efficient score is given by 
  \[
    S_{\text{eff}}\{C,G_C(Z)\}=S_\beta\{C,G_C(Z)\}-\Pi\big[S_\beta\{C,G_C(Z)\}\mid \Lambda \big],
  \]
  where $S_\beta=\E\big[S^F_\beta(Z)\mid C,G_C(Z)\big]$, and $\Lambda=\Lambda_{\psi}\oplus \Lambda_{\eta}\ \ (\Lambda_{\psi}\perp \Lambda_{\eta})$.
    \begin{align*}
      \Pi\big[S_\beta\{C,G_C(Z)\}\mid \Lambda \big]&=\Pi\big[S_\beta\{C,G_C(Z)\}\mid \Lambda_{\psi} \big]+\Pi\big[S_\beta\{C,G_C(Z)\}\mid \Lambda_{\eta} \big] \\
      &=\Pi\big[S_\beta\{C,G_C(Z)\}\mid \Lambda_{\eta} \big] \tag{\because $S_\beta\{C,G_C(Z)\}\perp \Lambda_2$}
    \end{align*}
    \item Recall $\Lambda_{\eta}=\{\E[\alpha^F(Z)\mid C,G_C(Z)] : \alpha^F(Z)\in\Lambda^F\}$, there exists $\alpha_{\text{eff}}^F(Z)\in \Lambda^F$, 
        \[
    \Pi\big[S_\beta\{C,G_C(Z)\}\mid \Lambda_{\eta} \big]=\E[\alpha_{\text{eff}}^F(Z)\mid C,G_C(Z)]
    \]
    \begin{align*}
      \therefore  S_{\text{eff}}\{C, G_C(Z)\}
&= S_{\beta}\{C, G_C(Z)\}
  - \Pi\!\big[ S_{\beta}\{C, G_C(Z)\} \,\big|\, \Lambda_{\eta} \big] \\
      &=
      E\!\left[ \, \big\{ S^{F}_{\beta}(Z) - \alpha^{F}_{\text{eff}}(Z) \big\}
   \,\big|\, C, G_C(Z) \right]
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}{Efficient score: Representation 2 (AIPWCC-based) : practical}
  \begin{itemize}
    \item Since the full-data efficient score $B^F(Z)\in \Lambda^{F\perp}$, we can apply $\mathcal{J}$.
  \[
    S_{\text{eff}}\{C,G_C(Z)\}=\mathcal{J}\big\{B_F^{\text{eff}}(Z)\big\}
    =I(C=\infty)\frac{B_F^{\text{eff}}(Z)}{\pi(\infty,Z)}-\Pi\!\left[I(C=\infty)\frac{B_F^{\text{eff}}(Z)}{\pi(\infty,Z)}\middle|\Lambda_2 \right].
  \]
  \item Clearly, this element lives in DR linear space $\mathcal{J}(\Lambda^{F\perp})$
  \item Now, we obtain two equivalent representations about the efficient score
  \[
  \mathbb{E}\!\left[\, S^{F}_{\beta}(Z) - \alpha^{F}_{\mathrm{eff}}(Z) \,\middle|\, C,\, G_{C}(Z) \right]
=
\frac{I(C=\infty)\, B^{F}_{\mathrm{eff}}(Z)}{\varpi(\infty, Z)}
\;-\;
\Pi\!\left[
  \frac{I(C=\infty)\, B^{F}_{\mathrm{eff}}(Z)}{\varpi(\infty, Z)}
  \,\middle|\, \Lambda_{2}
\right]
  \]
  \item For computation, this AIPWCC estimator is far more practical but still be difficult to construct. → Efficiency problem boils down to find $B^{F}_{\mathrm{eff}}(Z)$.
  \end{itemize}
\end{frame}

\begin{frame}{How to derive $B^{F}_{\mathrm{eff}}(Z)$}
  \begin{tcolorbox}[colframe=Cyan,title=Theorem 11.1]
    \begin{itemize}
      \item $B^{F}_{\mathrm{eff}}(Z)\in \Lambda^{F\perp}$ is the unique solution to
      \[
        \Pi\!\left[ \mathcal{M}^{-1}\{B^{F}(Z)\} \;\middle|\; \Lambda^{F\perp} \right]
        \;=\; S^{F}_{\mathrm{eff}}(Z),
      \]
    \end{itemize}
  \end{tcolorbox}
  \begin{itemize}
    \item Thereofore, solving this equation, we obtain $B^{F}_{\mathrm{eff}}(Z)$, then we can construct teh efficient score (Note : This is revisit)
    \[
    S_{\mathrm{eff}}\{C,G_C(Z)\}
=
\frac{I(C=\infty)\,B^{F}_{\mathrm{eff}}(Z)}{\varpi(\infty,Z)}
-
\Pi\!\left[
  \frac{I(C=\infty)\,B^{F}_{\mathrm{eff}}(Z)}{\varpi(\infty,Z)}
  \;\middle|\; \Lambda_2
\right],
    \]
  \end{itemize}
\end{frame}

\begin{frame}{Proof sketch of Lemma 11.1 : successive approximation}
  \begin{itemize}
    \item To construct $B^{F}_{\mathrm{eff}}(Z)$, define $D^{F}_{\mathrm{eff}}(Z)=\mathcal{M}^{-1}\{B^{F}_{\mathrm{eff}}(Z)\}$.
    \item Then $D^{F}_{\mathrm{eff}}(Z)$ is the solution to 
    \[
    S_{\mathrm{eff}}(Z)
= \Pi\!\bigl[h^{F}(Z)\,\big|\,\Lambda^{F\perp}\bigr]
+ \Pi\!\bigl[\mathcal{M}\{h^{F}(Z)\}\,\big|\,\Lambda^{F}\bigr]
= (I-\mathcal{Q})\{h^{F}(Z)\}
    \]
    where $\mathcal{Q}\{D^{F}_{\mathrm{eff}}(Z)\}
= \Pi\!\Bigl[(I-\mathcal{M})\{D^{F}_{\mathrm{eff}}(Z)\}\,\Big|\,\Lambda^{F}\Bigr]$
  \item $\mathcal{Q}$ is a contraction mapping, so $(I-\mathcal{Q})^{-1}$ uniquely exists.
  \item Finally, we can successively approximate $D^{F}_{\mathrm{eff}}(Z)=(\sum_{i=0}^{\infty}\mathcal{Q}^i)S^{F}_{\mathrm{eff}}(Z)$
  \end{itemize}
\end{frame}

\begin{frame}{Computational focus : $\mathcal{M}^{-1}$}
  \begin{tcolorbox}[colframe=Cyan,title=Theorem 11.2]
    \begin{itemize}
      \item Suppose the coarsening mechanism is \textbf{monotone}.
      \item For any $h^{F}(Z)\in\mathcal{H}^{F}$, the inverse operator $M^{-1}$ has the closed form
      \[
        M^{-1}\{h^{F}(Z)\}
        = \frac{h^{F}(Z)}{\varpi(\infty,Z)}
        - \sum_{r\neq\infty}
          \frac{\lambda_{r}\{G_{r}(Z)\}}{K_{r}\{G_{r}(Z)\}}
          \,\mathbb{E}\!\left[ h^{F}(Z)\mid G_{r}(Z)\right],
      \]
      where
      \[
        K_{r}(Z) = \Pr(C\ge r+1\mid Z), 
        \quad
        \lambda_{r}\{G_{r}(Z)\} = \Pr(C=r\mid C\ge r, Z).
      \]
      \item Hence, $M^{-1}$ can be computed explicitly in terms of the hazard and survival functions of the coarsening distribution.
    \end{itemize}
  \end{tcolorbox}
\end{frame}

\begin{frame}
  \begin{tcolorbox}[colframe=lightgray,title=Lemma 11.2]
    \begin{itemize}
      \item Let $\{D^{(k)}\}$ be defined recursively by
      \[
        D^{(k+1)}(Z)
        =
        \Pi\!\Big((I-M)D^{(k)}(Z)\,\Big|\,\mathcal{H}^{F}\Big)
        + S^{F}_{\mathrm{eff}}(Z),
      \]
      with $D^{(0)}$ arbitrary.
      \item Then
      \[
        D^{(k)} \;\longrightarrow\; D^{F}_{\mathrm{eff}}
        \quad\text{in $\mathcal{H}^{F}$ norm,}
      \]
      and
      \[
        B^{(k)}(Z)
        = \Pi\!\big(MD^{(k)}(Z)\,\big|\;\Lambda^{F\perp}\big)
        \;\longrightarrow\; B^{F}_{\mathrm{eff}}(Z).
      \]
      \item Thus, the successive approximation scheme converges to the efficient element $B^{F}_{\mathrm{eff}}(Z)$.
    \end{itemize}
  \end{tcolorbox}
\end{frame}

\section{11.2 Strategy for Obtaining Improved Estimators}

\begin{frame}{Estimation scheme}
  \begin{itemize}
    \item Consider finding a full-data estimating function $m(Z,\beta)$.
    \item Firstly, by MLE (or likelihood methods), we estimate $\hat{\psi}_n, \hat{\xi}_n^*$.
    \item Secondly, we calculate full-data efficient score $S^{\mathrm{eff}}_{F}(Z)$.
    \item By successive approximation, $\hat{B}^{F}_{\mathrm{eff}}(Z)=m(Z,^beta,\hat{\psi}_n, \hat{\xi}_n^*)$.
    \item Finally, we can estimate observed-data efficient score using
    \[
    S_{\mathrm{eff}}\{C,G_C(Z)\}
=
\frac{I(C=\infty)\,B^{F}_{\mathrm{eff}}(Z)}{\varpi(\infty,Z)}
-
\Pi\!\left[
  \frac{I(C=\infty)\,B^{F}_{\mathrm{eff}}(Z)}{\varpi(\infty,Z)}
  \;\middle|\; \Lambda_2
\right]
    \]
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{itemize}
    \item Now, we focus on the projection $\Pi\!\left[
  \frac{ I(C=\infty)\,m(Z,\beta) }{ \varpi(\infty,Z) }
  \,\Bigg|\, \Lambda_{2}
\right]$. \\
    → In some special coarsening mechanism, this projection is simplily written down.
    \item In actual implementation, we solve this adaptive estimating equation.
    \begin{align*}
\sum_{i=1}^{n} \Bigg(
  &\frac{ I(C_i=\infty)\, m(Z_i,\beta,\hat\psi_n,\hat\xi_n^{*}) }
        { \varpi(\infty,Z_i,\hat\psi_n) } \\[6pt]
  &- \frac{ I(C_i=\infty) }{ \varpi(\infty,Z_i,\hat\psi_n) }
     \Bigg[
       \sum_{r\neq\infty} \varpi\{r,G_r(Z_i),\hat\psi_n\}\,
         \E\!\Big\{ D^{(j)}(Z,\beta,\hat\psi_n,\hat\xi_n^{*})
             \,\Big|\, G_r(Z_i),\hat\xi_n^{*} \Big\}
     \Bigg] \\[6pt]
  &+ \sum_{r\neq\infty} I(C_i=r)\,
      \E\!\Big\{ D^{(j)}(Z,\beta,\hat\psi_n,\hat\xi_n^{*})
          \,\Big|\, G_r(Z_i),\hat\xi_n^{*} \Big\}
\Bigg) \;=\; 0
\end{align*}
  \end{itemize}
\end{frame}

\begin{frame}{locally efficient estimator for restricted moment model with monotone coarsening}
  \begin{itemize}
    \item Full data: $Z=(Y,X)$, Model: $Y=\mu(X,\beta)+\varepsilon$ with $\E(\varepsilon\mid X)=0$.
    \item Full-data efficient score: $S^F_{\text{eff}}(Z)=D^\top(X)V^{-1}(X)\varepsilon$, $D=\partial\mu/\partial\beta^\top, V(X)=\text{Var}(Y|X)$.
    \item Seek $B_F^{\text{eff}}(Z)=A(X)\varepsilon \in \mathcal{F}^{\perp}$ solving
    \[
      \Pi\!\big[M^{-1}\{A(X)\varepsilon\}\mid \mathcal{F}^{\perp}\big]=S^F_{\text{eff}}(Z).
    \]
    \item Leads to an integral equation for $A(X)$; exact solution rare; use numerical/approximate $A_{\text{imp}}$.
  \end{itemize}
\end{frame}

\begin{frame}{Approximate locally efficient estimator (practice)}
  \begin{itemize}
    \item Compute a numerically feasible $A_{\text{imp}}(X,\hat\beta,\hat\psi,\hat\xi^\ast)$.
    \item Plug into the AIPWCC efficient-score form:
    \[
      \sum_{i=1}^n\left[
        I(C_i=\infty)\frac{A_{\text{imp}}(X_i,\beta,\hat\psi,\hat\xi^\ast)\{Y_i-\mu(X_i,\beta)\}}{\pi(\infty,Y_i,X_i;\hat\psi)}
        +L_2\{C_i,G_{C_i}(Y_i,X_i),\beta,\hat\psi,\hat\xi^\ast\}
      \right]=0.
    \]
    \item Here $L_2$ is the projection term (closed form under monotone coarsening).
  \end{itemize}
  \vspace{2mm}
  \begin{tcolorbox}[title=Robustness note]
  The $A_{\text{imp}}$ route targets local efficiency but may be numerically heavy; when it’s too hard, fall back to the DR AIPWCC from Ch.~10.
  \end{tcolorbox}
\end{frame}

\section{11.3 Concluding Thoughts}

\begin{frame}{Summary}
  \begin{itemize}
    \item Adaptive estimation : Specified coarsening model → using posit model $\xi$ → fit $\xi_n^*$ 
    \item Inverse weighted method balances between simplicity of implementation and relative efficiency.
    \item AIPWCC estimator is more improved one in double robustness and gaining considerable efficiency at the cost of numerical implementaion. (successive approximation)
    \item Locally efficient estimator aims to find optimal full-data estimating function $B_{\text{eff}}^F(Z)$ as well as optimal augmentation. (difficult to implement) 
  \end{itemize}
\end{frame}

\section*{References}

\begin{frame}{References}
  \begin{itemize}
    \item Tsiatis, A. (2006). \emph{Semiparametric Theory and Missing Data}. Springer (Chs. 10–11).
    \item A.W. van der Vaart (1998). \emph{Asymptotic Statistics}. Cambridge University Press.
  \end{itemize}
\end{frame}

\end{document}
